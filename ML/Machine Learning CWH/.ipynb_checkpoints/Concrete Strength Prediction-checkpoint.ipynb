{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Problem Statement: Concrete Strength Prediction </h3>\n",
    "\n",
    "<h5>Objective:</h5>\n",
    "\n",
    "<p>To predict the concrete strength using the data available in file \"concrete.csv\". Apply feature engineering and model tuning to obtain a score above 85%.</p>\n",
    "\n",
    "<h5>Resources Available: </h5>\n",
    "\n",
    "<p>The data for this project is available in file <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive\">https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive</a>. The same has been shared along with the course content.</p>\n",
    "\n",
    "<h5>Attribute Information:</h5>\n",
    "<p>Given are the variable name, variable type, the measurement unit, and a brief description. The concrete compressive strength is the regression problem. The order of this listing corresponds to the order of numerals along the rows of the database.</p>\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <th></th>\n",
    "        <th>Name</th>\n",
    "        <th>Data Type</th>\n",
    "        <th>Measurement</th>\n",
    "        <th>Description</th>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>1.</td>\n",
    "            <td>Cement (cement)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "            <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>2.</td>\n",
    "            <td>Blast Furnace Slag (slag)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "             <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>3.</td>\n",
    "            <td>Fly Ash (ash)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "            <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>4.</td>\n",
    "            <td>Water(water)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "             <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>5.</td>\n",
    "            <td>Superplasticizer (superplastic)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "            <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>6.</td>\n",
    "            <td>Coarse Aggregate (coarseagg)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "            <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>7.</td>\n",
    "            <td>Fine Aggregate (fineagg)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>kg in a m3 mixture</td>\n",
    "            <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>8.</td>\n",
    "            <td>Age(age)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>Day (1~365)</td>\n",
    "            <td>Input Variable</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>9.</td>\n",
    "            <td>Concrete compressive strength(strength)</td>\n",
    "            <td>quantitative</td>\n",
    "            <td>MPa</td>\n",
    "            <td>Output Variable</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "   </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'concrete.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# reading the data from csv and viewing the head records\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m concrete_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconcrete.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m concrete_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'concrete.csv'"
     ]
    }
   ],
   "source": [
    "# reading the data from csv and viewing the head records\n",
    "concrete_df = pd.read_csv('concrete.csv')\n",
    "concrete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`The data has no categorical features`\n",
    "\n",
    "`The data has features with value 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the dataframe shape\n",
    "concrete_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`The data has 1030 rows with 9 columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the data types columns\n",
    "concrete_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`Age column has integer values where as rest of the columns are float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for statistical summary like mean, median, range, skewness and inter quantile range\n",
    "\n",
    "summary_df = concrete_df.describe().transpose()\n",
    "summary_df['range'] = summary_df['max'] - summary_df['min']\n",
    "summary_df['skeness'] = concrete_df.skew()\n",
    "summary_df['median'] = concrete_df.median()\n",
    "summary_df['IQR'] = concrete_df.quantile(0.75) - concrete_df.quantile(0.25)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. slag, ash and superplatic features have minimum value as 0\n",
    "2. It looks like age has right skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null data \n",
    "\n",
    "concrete_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. There are no null/missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying outliers\n",
    "IQR = summary_df['IQR']\n",
    "Q1 = concrete_df.quantile(0.25)\n",
    "Q3 = concrete_df.quantile(0.75)\n",
    "\n",
    "((concrete_df< Q1 - 1.5 * IQR) | (concrete_df > Q3 + 1.5* IQR)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`age has highest number of outliers`\n",
    "\n",
    "`we have outliers in target variable as well`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "concrete_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "`We have 25 duplicate records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicate rows\n",
    "concrete_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for columns with 0 value\n",
    "concrete_df[concrete_df==0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "`There are 470 rows with slag value as 0, 541 rows with ash value as 0 and 378 rows with superplastic as 0. We cannot consider these rows as null/missing values, as we can have concrete without adding these substances. Hence these are valid rows`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution plots for each column\n",
    "def plot_univariate_graphs(df):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    count=1\n",
    "    for col in df.columns:\n",
    "        plt.subplot(4,3,count)\n",
    "        sns.distplot(df[col], kde=True)\n",
    "        count+=1\n",
    "    plt.show()\n",
    "    \n",
    "plot_univariate_graphs(concrete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. slag, ash, superplastic distributions have more than one gaussians\n",
    "2. age distribution is very right skewed\n",
    "3. water, cement, coarseagg and fineagg seems fairly normally distributed\n",
    "4. strength is normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plot between superplastic and water\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.scatterplot(x='superplastic', y='water',data=concrete_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. As the content of superplastic increases, amount of water decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the scatterplot between target variable and each feature to know the relationship of feature to target\n",
    "def plot_bivariate_analysis_feature_vs_target(df):\n",
    "    plt.figure(figsize=(40,70))\n",
    "    count=1\n",
    "    for col in df.columns:\n",
    "        if col == 'strength':\n",
    "            continue\n",
    "        plt.subplot(5,2,count)\n",
    "        sns.scatterplot(df[col], df['strength'])\n",
    "        count += 1\n",
    "    plt.show()\n",
    "plot_bivariate_analysis_feature_vs_target(concrete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. As cement increases the concrete strength increases\n",
    "2. As superplastic increases the concrete strength seems to increase as well\n",
    "3. Other than cement and superplastic, there is no linear relationship between the strength and the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots on the entire dataset\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.boxplot(data=concrete_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. Age has more number of outliers\n",
    "2. slag, water, superplastic, fineagg and strength has some outliers\n",
    "3. There are no outliers in cement, ash, coarseagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotting pairplot to know the relationship between features to target as well as within the features\n",
    "\n",
    "sns.pairplot(concrete_df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. As we increase fineagg substance, it looks like we require less water. Similar trend with water and superplastic\n",
    "2. As cement increases strength seems increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting corrleation using heat map\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.heatmap(concrete_df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. There is a positive correlation between cement and strength\n",
    "2. age and superplastic has positive correlation with strength\n",
    "3. There is a strong negative correlation between water and superplastic\n",
    "4. There is a strong negative correlation between water and fineagg\n",
    "5. ash and superplastic have positive correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "<p>As per the link <a href=\"https://www.cement.org/cement-concrete/how-concrete-is-made#:~:text=A%20properly%20designed%20mixture%20possesses,15%20to%2020%20percent%20water\">concrete-mixture-proportions</a>, concrete is a mixture of paste and aggregates, or rocks. The paste, composed of cement and water. </p>\n",
    "\n",
    "<p> The quality of the paste  determines the character of the concrete. The strength of the paste, in turn, depends on the ratio of water to cement. The water-cement ratio is the weight of the mixing water divided by the weight of the cement. High-quality concrete is produced by lowering the water-cement ratio <p>\n",
    "    \n",
    "<p> Hence, creating a new feature <b><i>water_to_cement_ratio</i></b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating water cement ratio\n",
    "concrete_df['water_cement_ratio'] = concrete_df['water'] / concrete_df['cement']\n",
    "concrete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scatterplot between the newly created feature vs the target\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.scatterplot(x='water_cement_ratio', y='strength',data=concrete_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`lesser the water_cement_ration, higher the strength`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the correlation between water_cement_ratio and strength\n",
    "\n",
    "concrete_df['water_cement_ratio'].corr(concrete_df['strength'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`As we have seen in the scatterplot, there is a negative correlation between water_cement_ratio and strength`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "`As we don't know what features are more important at this moment, i am not going to drop any features right now`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in to train and test\n",
    "X = concrete_df.drop('strength', axis=1)\n",
    "y = concrete_df['strength']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the outliers in xtrain and ytrain, treat those outliers and make the data ready for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that return outliers\n",
    "def get_outliers(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outlier_per_feature = ((df< Q1 - 1.5 * IQR) | (df > Q3 + 1.5* IQR)).sum() \n",
    "    return outlier_per_feature\n",
    "\n",
    "#find the outliers in train data\n",
    "outlier_per_feature = get_outliers(X_train)\n",
    "outlier_per_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. age has highest outliers\n",
    "2. The newly extracted feature water_cement_ratio also has outliers\n",
    "\n",
    "\n",
    "`Though age has outliers, i think they are valid values as age refers to the age of the concret. So we should keep the age values as is`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the outliers before fitting the model\n",
    "\n",
    "columns_with_outliers = ['fineagg', 'slag', 'superplastic', 'water_cement_ratio', 'water']\n",
    "\n",
    "#Function that treats the outliers \n",
    "def treat_outliers(df, columns_with_outliers, target_variable):\n",
    "    if target_variable:\n",
    "        q1 = df.quantile(0.25)\n",
    "        q3 = df.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        upper_wisker = q3 + 1.5 * iqr\n",
    "        lower_wisker = q1 - 1.5 * iqr\n",
    "        #Replace the lower-end outliers with lower wisker\n",
    "        df[df < lower_wisker] = lower_wisker\n",
    "\n",
    "         #Replace the upper-end outliers with upper wisker\n",
    "        df[df > upper_wisker] = upper_wisker\n",
    "    else:\n",
    "        for col in columns_with_outliers:\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper_wisker = q3 + 1.5 * iqr\n",
    "            lower_wisker = q1 - 1.5 * iqr\n",
    "            #Replace the lower-end outliers with lower wisker\n",
    "            df[col] = np.where(df[col] < lower_wisker, lower_wisker, df[col])\n",
    "\n",
    "             #Replace the upper-end outliers with upper wisker\n",
    "            df[col] = np.where(df[col] > upper_wisker, upper_wisker, df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat outliers in X_train\n",
    "treat_outliers(X_train, columns_with_outliers, False)\n",
    "outlier_per_feature = get_outliers(X_train)\n",
    "outlier_per_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find if we have outliers in target train data\n",
    "target_outliers = get_outliers(y_train)\n",
    "target_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`There are 10 outliers in target train data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat outliers in y_train\n",
    "treat_outliers(y_train, None, True)\n",
    "target_outliers = get_outliers(y_train)\n",
    "target_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the simple linear regression\n",
    "\n",
    "simple_linreg = LinearRegression()\n",
    "simple_linreg.fit(X_train, y_train)\n",
    "\n",
    "train_r2 = simple_linreg.score(X_train, y_train)\n",
    "test_r2 = simple_linreg.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, simple_linreg.predict(X_test))\n",
    "\n",
    "# Create a dataframe with train, test scores and mean squared error\n",
    "scores_df = pd.DataFrame(columns=['train score', 'test score', 'mean squared error'])\n",
    "scores_df.loc['Simple Linear Regression'] = [train_r2, test_r2, mse]\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fit polynomial features on a given model and return the scores dataframe\n",
    "\n",
    "def doPolynomialRegression(model, train_x, train_y, test_x, test_y, interaction_only, scores_df):\n",
    "    # Iterate through degrees 2 to 5\n",
    "    for degree in [2, 3, 4, 5]:\n",
    "        poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only)\n",
    "        \n",
    "        #create train and test polynomial features of given degree\n",
    "        X_train_poly = poly.fit_transform(train_x)\n",
    "        X_test_poly = poly.fit_transform(test_x)\n",
    "\n",
    "        #fit the model on train data with polynomial features\n",
    "        model.fit(X_train_poly, train_y)\n",
    "        y_predict = model.predict(X_test_poly)\n",
    "        \n",
    "        #calculate train,test scores and mean squared error\n",
    "        train_r2 = model.score(X_train_poly, train_y)\n",
    "        test_r2 = model.score(X_test_poly, test_y)\n",
    "        mse = mean_squared_error(test_y, y_predict)\n",
    "        \n",
    "        #update the scores_df with calculated metrics\n",
    "        scores_df.loc['Linear Regression With Degree {}'.format(degree)] = [train_r2, test_r2, mse]\n",
    "        \n",
    "    return scores_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_only = True\n",
    "\n",
    "polynomial_regressor = LinearRegression()\n",
    "\n",
    "# doing linear regression using polynomial features\n",
    "doPolynomialRegression(polynomial_regressor,X_train, y_train, X_test, y_test, interaction_only, scores_df)\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "1. Since there exists non-linear relationship, we should use Polynomial features to avoid under fit. If we observe the metrics of simple linear regression, the model only accomidated 59% of the variance in the train data which is a clear indication of underfit.\n",
    "\n",
    "\n",
    "2. The test R^2 value increased with degree of polynomial features up until the degree 3. The test R^2 of the models with degree greter than 3 dropped significantly which is an overfit scenario. Also the mean squared error for the models with degree greater than 3 increased significantly. \n",
    "\n",
    "<b>conclusion on complexity of the model interms of parameters: </b>\n",
    "\n",
    "`We should use polynomial features with degree 3 as it has better train and test scores when compared to quadratic and simple linear regression models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to capture the results of each model and their metrics\n",
    "\n",
    "result_df = pd.DataFrame(columns=['train score', 'test score', 'mean squared error', 'k-fold-cv mean', 'k-fold-cv std'])\n",
    "\n",
    "# evaluate_models function does the following:\n",
    "\n",
    "#    1. It takes list of models with train and test data.\n",
    "#    2. iterates through each model\n",
    "#    3. splits the train data into train and validate. \n",
    "#    4. fits the model on train data\n",
    "#    5. does the k-fold cross validation on validation data\n",
    "#    6. calculates the train, test R^2 values, mean squared error, k-fold cross validation mean and standard deviation\n",
    "#    7. updates all the above calculated metrics in the results_df per model and returns the results_df\n",
    "\n",
    "def evaluate_models(model_list):  \n",
    "    # define kfold object\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=7)\n",
    "    \n",
    "    #iterate though list of models\n",
    "    for model_obj in model_list:\n",
    "        #get the model instance from model_obj\n",
    "        model_instance = model_obj['model']\n",
    "        #get train,test data from model_obj\n",
    "        train_x = model_obj['xtrain']\n",
    "        train_y = model_obj['ytrain']\n",
    "        xtest = model_obj['xtest']\n",
    "        ytest = model_obj['ytest']\n",
    "        \n",
    "        \n",
    "        #split the train data in to train and validate\n",
    "        xtrain, xval, ytrain, yval = train_test_split(train_x, train_y, test_size=0.2, random_state=7)\n",
    "        \n",
    "        #fit the model against train data\n",
    "        model_instance.fit(xtrain, ytrain)\n",
    "        \n",
    "        # calculate train R^2\n",
    "        train_r2 = model_instance.score(xtrain, ytrain)\n",
    "        \n",
    "        # perform kfold cross validation with validation data\n",
    "        cv_result = cross_val_score (model_instance, xval, yval, cv=kfold)\n",
    "        \n",
    "        # calculate test R^2\n",
    "        test_r2 = model_instance.score(xtest, ytest)\n",
    "        \n",
    "        #calulate mean sqaured error\n",
    "        y_predict = model_instance.predict(xtest)\n",
    "        mse = mean_squared_error(ytest, y_predict)\n",
    "\n",
    "        #update the calculated metrics per model\n",
    "        result_df.loc[model_obj['name']]= [train_r2, test_r2, mse, cv_result.mean(), cv_result.std() ]\n",
    "\n",
    "    return result_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_list_of_models function does the following:\n",
    "#    1. Creates list of models with train,test data\n",
    "#    2. For Regularization models like Ridge and lasso, it uses polynomial features with degree 3\n",
    "#    3. It creates two sets of models for decision and ensemble models, with polynomial features and \n",
    "#     without polynomial features\n",
    "\n",
    "def get_list_of_models(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Create polynomial features for X_train and X_test with degree 3\n",
    "    poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "    \n",
    "    model_list = [\n",
    "        \n",
    "        # Regularization models with polynomial features of degree 3\n",
    "        \n",
    "        {'model': Ridge(alpha=.3, random_state=7), 'name': 'Ridge Regression With Degree 3',\n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        {'model': Lasso(alpha=0.2, random_state=7), 'name': 'Lasso Regression With Degree 3',\n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        \n",
    "        # Decision Tree and ensemble models with polynomial features of degree 3\n",
    "        \n",
    "        {'model': DecisionTreeRegressor(random_state=7), 'name': 'DecisionTree Regressor With polynomial',\n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        {'model': BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=7), n_jobs=4, random_state=7), \n",
    "                 'name': 'Bagging Regressor With polynomial', \n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        {'model': AdaBoostRegressor(base_estimator=DecisionTreeRegressor(random_state=7), random_state=7), \n",
    "                 'name': 'AdaBoost Regressor With polynomial', \n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        {'model': GradientBoostingRegressor(random_state=7), 'name': 'GradientBoost Regressor With polynomial', \n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        {'model': RandomForestRegressor(random_state=7), 'name': 'RandomForest Regressor With polynomial', \n",
    "                 'xtrain': X_train_poly, 'ytrain': y_train, 'xtest': X_test_poly, 'ytest': y_test },\n",
    "        \n",
    "        # Decision Tree and ensemble models without polynomial features\n",
    "        \n",
    "        {'model': DecisionTreeRegressor(random_state=7), 'name': 'Decision Tree Regressor',\n",
    "                 'xtrain': X_train, 'ytrain': y_train, 'xtest': X_test, 'ytest': y_test },\n",
    "        {'model': BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=7), n_jobs=4, random_state=7), \n",
    "                 'name': 'Bagging Regressor', \n",
    "                 'xtrain': X_train, 'ytrain': y_train, 'xtest': X_test, 'ytest': y_test },\n",
    "        {'model': AdaBoostRegressor(base_estimator=DecisionTreeRegressor(random_state=7), random_state=7), \n",
    "                 'name': 'AdaBoost Regressor', \n",
    "                 'xtrain': X_train, 'ytrain': y_train, 'xtest': X_test, 'ytest': y_test },\n",
    "        {'model': GradientBoostingRegressor(random_state=7), 'name': 'GradientBoost Regressor', \n",
    "                 'xtrain': X_train, 'ytrain': y_train, 'xtest': X_test, 'ytest': y_test },\n",
    "        {'model': RandomForestRegressor(random_state=7), 'name': 'RandomForest Regressor', \n",
    "                 'xtrain': X_train, 'ytrain': y_train, 'xtest': X_test, 'ytest': y_test }\n",
    "    ]\n",
    "    \n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the list of models\n",
    "model_list = get_list_of_models(X_train, y_train, X_test, y_test)\n",
    "#evaluate each model\n",
    "result_df = evaluate_models(model_list)\n",
    "\n",
    "#look for metrics of each model\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlike Linear models, DecisionTree and other Ensemble Models considers interaction terms. We don't need to specify external interaction terms. If we look at the above result dataframe, there is no much impact adding polynomial features on Decisiontree and other ensemble models. \n",
    "\n",
    "### I am going to use GradientBoostingRegressor and RandomForestRegressor from here on. Hence I am going to ignore polynomial features as more number of features increases the complexity of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feauture Elimination - RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=15)\n",
    "\n",
    "rf_rfecv = RFECV(estimator=RandomForestRegressor(random_state=7), step=1, cv=kfold)\n",
    "rf_rfecv.fit(X_train, y_train)\n",
    "print('Optimal number of features: {}'.format(rf_rfecv.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot feature vs importance\n",
    "def plot_feature_vs_importance(features, importances, title):\n",
    "    feature_importance_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.barh(y=feature_importance_df['feature'], width=feature_importance_df['importance'], color='#1976D2')\n",
    "    plt.title(title, fontsize=20, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Importance', fontsize=14, labelpad=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the important features using recursive feature elimination and plot feature vs importance graph\n",
    "rf_rfe_importances = rf_rfecv.estimator_.feature_importances_\n",
    "rf_important_features = X_train.columns[np.where(rf_rfecv.support_ == True)[0]]\n",
    "plot_feature_vs_importance(rf_important_features, rf_rfe_importances, 'RF Recursive Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`As per RandomForestRegressor, all 9 features are important in predicting concrete strength`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feauture Elimination - GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using recursive feature elimination technique\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=15)\n",
    "\n",
    "gbr_rfecv = RFECV(estimator=GradientBoostingRegressor(random_state=7), step=1, cv=kfold)\n",
    "gbr_rfecv.fit(X_train, y_train)\n",
    "print('Optimal number of features: {}'.format(gbr_rfecv.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the important features using recursive feature elimination and plot feature vs importance graph\n",
    "gbr_rfe_importances = gbr_rfecv.estimator_.feature_importances_\n",
    "gbr_important_features = X_train.columns[np.where(gbr_rfecv.support_ == True)[0]]\n",
    "\n",
    "print(gbr_rfe_importances)\n",
    "plot_feature_vs_importance(gbr_important_features, gbr_rfe_importances, 'GBR Recursive Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "`As per GradientBoostingRegressor, ash and coarseagg doesn't seem contributing in predicting the concrete strength`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns array that can be dropped as per RecursiveFeatureElimination using RandomForestRegressor\n",
    "columns_to_drop = X_train.columns[np.where(rf_rfecv.support_ == False)[0]]\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split original data into train and test, drop the features that are not important as per\n",
    "# Recursive Feature Elimination\n",
    "def split_data_train_test(df, columns_to_drop):\n",
    "    columns_to_drop.append('strength')\n",
    "    X = concrete_df.drop(columns_to_drop, axis=1)\n",
    "    y = concrete_df['strength']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=7)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_data_train_test(concrete_df,[])\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the outliers in xtrain and ytrain, treat those outliers and make the data ready for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the outliers in train\n",
    "def print_outliers_in_train(X_train, y_train):\n",
    "    outliers_in_xtrain = get_outliers(X_train)\n",
    "    outliers_in_ytrain = get_outliers(y_train)\n",
    "    print('Outliers in xtrain: ')\n",
    "    print(outliers_in_xtrain)\n",
    "    print('\\n')\n",
    "    print('Outliers in ytrain: ')\n",
    "    print(outliers_in_ytrain)\n",
    "\n",
    "print_outliers_in_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat outliers in X_train, X_val and y_train\n",
    "\n",
    "def treat_outliers_in_train(X_train, y_train):\n",
    "    treat_outliers(X_train, ['slag', 'water', 'superplastic','fineagg', 'water_cement_ratio'], False)\n",
    "    treat_outliers(y_train, None, True)\n",
    "    \n",
    "\n",
    "treat_outliers_in_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune RandomForestModel using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the parameter distribution for RandomSearchCV with RandomForestRegressor parameters\n",
    "rs_params_dist = {   \n",
    "                'max_depth': randint(3,8),\n",
    "                'max_features': ['auto', 'sqrt'],\n",
    "                'bootstrap': [True],\n",
    "                'criterion': ['mse'],\n",
    "                'n_estimators': randint(100, 300)\n",
    "                }\n",
    "\n",
    "rf_rs_cv = RandomizedSearchCV(RandomForestRegressor(random_state=7), \n",
    "                              param_distributions=rs_params_dist, \n",
    "                              n_iter=20, cv=10, \n",
    "                              n_jobs=4,\n",
    "                              random_state=7)\n",
    "# fit the train data\n",
    "rf_rs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best params\n",
    "rf_rs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final result dataframe with train, test and mean sequared error metrics per model\n",
    "final_results_df = pd.DataFrame(columns=['train_score', 'test_score', 'mean_squared_error'])\n",
    "\n",
    "y_predict = rf_rs_cv.predict(X_test)# It will use the model best hyper parameters when doing predict\n",
    "\n",
    "\n",
    "final_results_df.loc['RandomForest_With_RSCV'] = [\n",
    "                                                rf_rs_cv.score(X_train, y_train),\n",
    "                                                rf_rs_cv.score(X_test, y_test),\n",
    "                                                mean_squared_error(y_test, y_predict)\n",
    "                                            ]\n",
    "\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look For columns that needs to be dropped as per RecursiveFeatureElimination using GradientBoostingRegressor\n",
    "columns_to_drop = X_train.columns[np.where(gbr_rfecv.support_ == False)[0]]\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split original data in to train and test, drop columns as per RecursiveFeatureElimination using GradientBoostingRegressor\n",
    "X_train, y_train,X_test, y_test = split_data_train_test(concrete_df,['ash', 'coarseagg'])\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the outliers in xtrain and ytrain, treat those outliers and make the data ready for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print outliers in train data\n",
    "print_outliers_in_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat Outlier in train data\n",
    "treat_outliers_in_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the parameter distribution for RandomSearchCV for GradientboostingRegressor\n",
    "rs_params_dist = {\n",
    "                    'loss':['ls', 'lad'],\n",
    "                    'learning_rate': uniform(0.001,1.00),\n",
    "                    'criterion': ['friedman_mse', 'mse'],\n",
    "                    'n_estimators': randint(100, 300),\n",
    "                    'max_depth': randint(3,7)\n",
    "                }\n",
    "\n",
    "\n",
    "gb_rs_cv = RandomizedSearchCV(GradientBoostingRegressor(random_state=7), \n",
    "                              param_distributions=rs_params_dist, \n",
    "                              n_iter=20, cv=10, \n",
    "                              n_jobs=4,\n",
    "                              random_state=7)\n",
    "#fit the train data\n",
    "gb_rs_cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best params\n",
    "gb_rs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = gb_rs_cv.predict(X_test)# It will use the model with best hyper parameters when doing predict\n",
    "\n",
    "\n",
    "final_results_df.loc['GradientBoost_With_RSCV'] = [\n",
    "                                                gb_rs_cv.score(X_train, y_train),\n",
    "                                                gb_rs_cv.score(X_test, y_test),\n",
    "                                                mean_squared_error(y_test, y_predict)\n",
    "                                            ]\n",
    "\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Of R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> R2 explains the amount of variance explained in the data. In otherword, how well the model fit the data. It will have values between 0 to 1</li>\n",
    "    <li> Higher value of R2 means, more variance is accounted by the model. In other words, if the model has higher R2 value (closer to 1) then the model performs good in predicting the target,  in our case the concrete strength. If the model has low R2 value that model won't be a good predictor of target </li>\n",
    "\n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostRegressor has R2 value as 94% for test data.  So our final model is going to be GradientBoostRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference about the data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_95_percent_confidence_interval(model):\n",
    "    # get the mean validation score from cv_results of best index\n",
    "    mean_validation_score = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "    # get the standard deviation of validation from cv_results of best index\n",
    "    standard_deviation_validation = model.cv_results_['std_test_score'][model.best_index_]\n",
    "    z = 1.96 # the z-socre for 95% confidence interval is 1.96\n",
    "    confidence_interval = '[{}, {}]'.format(\n",
    "                                    (mean_validation_score - z * standard_deviation_validation),\n",
    "                                    (mean_validation_score + z * standard_deviation_validation)\n",
    "                                    )\n",
    "    return confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_95_percent_confidence_interval = compute_95_percent_confidence_interval(gb_rs_cv)\n",
    "_95_percent_confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We did exploratory data analysis (EDA), identified the outliers in the data, cleaned the data to make it ready for the model.` \n",
    "\n",
    "`During EDA, we identified that some of the features could be a good predictors of the target based on thier relationship (We identified this relationship by using scatter plots and correlation functions). For instance, water_cement_ratio feature has a negative correlation with target variable and we knew that it will be a good predictor of concrete strength and we found that our initial inference is true on the final model.`\n",
    "\n",
    "`The final model has accuracy between 87% to 94% with a confidence of 95%.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations:\n",
    "\n",
    "Based on our best model, we can make the following recommendations\n",
    "\n",
    "1. water_cement_ratio, age, slag, superplastic, water, cement, fineagg are best features with which we can predict concrete strength\n",
    "\n",
    "2. We can ignore ash optional substance and coarseagg as they don't seem adding much value towards concrete strength\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_rs_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('concrete_strength_trained_model.pkl', 'wb') as m:\n",
    "    pickle.dump(gb_rs_cv.best_estimator_, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('concrete_strength_trained_model.pkl', 'rb') as m:\n",
    "    loaded_model = pickle.load(m)\n",
    "loaded_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
